{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing homemade layers\n",
    "\n",
    "This notebook contains tests for all the homemade layers. The tests takes some samples from the mni-\n",
    "st dataset as a minibatch and compares the results from running this minibatch through the test CNN\n",
    "to running the same minibatch through the homemade layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing relevant libraries\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "from CNN_small_architecture import CNNSmall\n",
    "from CNN_layers import conv_homemade\n",
    "from CNN_layers import maxpool_homemade\n",
    "from CNN_layers import batchnorm_homemade\n",
    "from CNN_layers import linear_layer_homemade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(num):\n",
    "    if num == 1:\n",
    "        return torch.tensor(np.array([1., 0.]))\n",
    "    else:\n",
    "        return torch.tensor(np.array([0., 1.]))\n",
    "\n",
    "MNIST_test = datasets.MNIST(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "test_set = [[data[0], tokenize(data[1])] for data in MNIST_test if data[1] in [1,2]]\n",
    "\n",
    "batch_size = 2\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a 4-dimensional tensor from the dataloader (batch_size x channel_size x height x width)\n",
    "# and turns it into a list of lists of (height x width) numpy arrays\n",
    "def transform_input(input_batch):\n",
    "    return list(list(input_batch.detach().numpy()))\n",
    "\n",
    "# compares two lists of batches with same amount of channels \n",
    "# and returns the summed absoute loss for each batch over the channels\n",
    "def compare(x, y):\n",
    "    b_diff = []\n",
    "    batch_size = len(x)\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        c_diff = []\n",
    "        channels = len(x[b])\n",
    "\n",
    "        for c in range(channels):\n",
    "            diff = np.sum(np.absolute(x[b][c] - y[b][c]))\n",
    "            c_diff.append(diff)\n",
    "        \n",
    "        b_diff.append(c_diff)\n",
    "    \n",
    "    return b_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_original = CNNSmall()\n",
    "model_original.eval()\n",
    "\n",
    "path = \"CNN_small\"\n",
    "load = True\n",
    "\n",
    "if load and os.path.isfile(path):\n",
    "    model_original.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "input_original_test, label_test = next(iter(test_loader))\n",
    "print(np.array(input_original_test).shape)\n",
    "input_homemade_test = transform_input(input_batch=input_original_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv_homemade(model_conv):\n",
    "    weights = model_conv.weight\n",
    "    biases = model_conv.bias.detach().numpy()\n",
    "\n",
    "    out_c, in_c, r, c = weights.shape\n",
    "    conv1_filters = []\n",
    "\n",
    "    for f in range(out_c):\n",
    "        filter_ = []\n",
    "        kernels = []\n",
    "\n",
    "        for kernel in list(weights[f,:,:,:]):\n",
    "            kernels.append(kernel.detach().numpy())\n",
    "\n",
    "        filter_.append(kernels)\n",
    "        filter_.append(biases[f])\n",
    "        conv1_filters.append(filter_)\n",
    "    \n",
    "    return conv_homemade.Conv(filters=conv1_filters, in_channels=in_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batchnorm_homemade(model_batchnorm):\n",
    "    weights = model_batchnorm.weight\n",
    "    biases = model_batchnorm.bias\n",
    "    running_mean = model_batchnorm.running_mean\n",
    "    running_var = model_batchnorm.running_var\n",
    "\n",
    "    return batchnorm_homemade.BatchNorm(weights=weights, biases=biases, running_mean = running_mean, running_var = running_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_maxpool_homemade(model_maxpool):\n",
    "    kernel_size = model_maxpool.kernel_size\n",
    "    stride = model_maxpool.stride\n",
    "    if type(model_maxpool.padding) == int:\n",
    "        padding = (model_maxpool.padding, model_maxpool.padding)\n",
    "    else:\n",
    "        padding = model_maxpool.padding\n",
    "    \n",
    "    return maxpool_homemade.MaxPool(kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRIDE ER PRÆDEFINERET\n",
      "STRIDE ER PRÆDEFINERET\n"
     ]
    }
   ],
   "source": [
    "conv1_homemade = create_conv_homemade(model_conv= model_original.conv1)\n",
    "batchnorm1_homemade = create_batchnorm_homemade(model_batchnorm= model_original.batchNorm1)\n",
    "maxpool1_homemade = create_maxpool_homemade(model_maxpool= model_original.maxPool1)\n",
    "conv2_homemade = create_conv_homemade(model_conv= model_original.conv2)\n",
    "batchnorm2_homemade = create_batchnorm_homemade(model_batchnorm= model_original.batchNorm2)\n",
    "maxpool2_homemade = create_maxpool_homemade(model_original.maxPool2)\n",
    "linear_homemade = linear_layer_homemade.linear_layer(model_original.lin.weight,model_original.lin.bias,2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the first convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional layer 1 error over out channels: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[2.4508228573666413e-06, 2.7754352047160147e-06, 2.4542343283751045e-06],\n",
       " [3.954176872073001e-06, 5.6260658496676275e-06, 4.12328020808761e-06]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# homemade conv1 filter on test\n",
    "out_homemade = conv1_homemade(input_homemade_test)\n",
    "# original conv1 filter on test\n",
    "out_original = model_original.conv1(input_original_test)\n",
    "\n",
    "print(\"Convolutional layer 1 error over out channels: \")\n",
    "compare(out_homemade, transform_input(input_batch = out_original))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the first Batchnorm layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchNorm layer 1 error over out channels: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[5.3554773330688477e-05, 4.4269487261772156e-05, 5.099223926663399e-05],\n",
       " [6.334856152534485e-05, 7.360382005572319e-05, 6.171129643917084e-05]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# homemade batchNorm1 filter on test\n",
    "out_homemade = batchnorm1_homemade(input_batch=out_homemade)\n",
    "# original batchNorm1 filter on test\n",
    "out_original = model_original.batchNorm1(out_original)\n",
    "\n",
    "print(\"BatchNorm layer 1 error over out channels: \")\n",
    "compare(out_homemade, transform_input(input_batch = out_original))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the first Maxpool layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxPool layer 1 error over out channels: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1.4118850231170654e-05, 1.1965632438659668e-05, 1.4308840036392212e-05],\n",
       " [1.714937388896942e-05, 2.1882355213165283e-05, 1.6798265278339386e-05]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# homemade MaxPool filter on test\n",
    "out_homemade = maxpool1_homemade(input_batch=out_homemade)\n",
    "# original MaxPool filter on test\n",
    "out_original = model_original.maxPool1(out_original)\n",
    "\n",
    "print(\"MaxPool layer 1 error over out channels: \")\n",
    "compare(out_homemade, transform_input(input_batch = out_original))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the second Convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxPool layer 1 error over out channels: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[6.839803987607196e-06,\n",
       "  1.13506730272106e-05,\n",
       "  8.365254015374468e-06,\n",
       "  1.1760292614348966e-05,\n",
       "  8.43778021558661e-06],\n",
       " [1.4010783349727163e-05,\n",
       "  1.4851634311224826e-05,\n",
       "  1.2323523619323096e-05,\n",
       "  1.5419261641030912e-05,\n",
       "  1.422003913834069e-05]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: test the second convolutional layer\n",
    "\n",
    "# homemade batchNorm1 filter on test\n",
    "out_homemade = conv2_homemade(input_batch=out_homemade)\n",
    "# original batchNorm1 filter on test\n",
    "out_original = model_original.conv2(out_original)\n",
    "\n",
    "print(\"MaxPool layer 1 error over out channels: \")\n",
    "compare(out_homemade, transform_input(input_batch = out_original))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the second Batchnorm layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxPool layer 1 error over out channels: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1.0855495929718018e-05,\n",
       "  2.0815059542655945e-05,\n",
       "  1.3127923011779785e-05,\n",
       "  1.8320977687835693e-05,\n",
       "  1.2755393981933594e-05],\n",
       " [2.2135674953460693e-05,\n",
       "  2.8287991881370544e-05,\n",
       "  1.8671154975891113e-05,\n",
       "  2.382509410381317e-05,\n",
       "  2.467632293701172e-05]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# homemade batchNorm1 filter on test\n",
    "out_homemade = batchnorm2_homemade(input_batch=out_homemade)\n",
    "# original batchNorm1 filter on test\n",
    "out_original = model_original.batchNorm2(out_original)\n",
    "\n",
    "print(\"MaxPool layer 1 error over out channels: \")\n",
    "compare(out_homemade, transform_input(input_batch = out_original))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the second Maxpool layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxPool layer 1 error over out channels: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[2.0116567611694336e-06,\n",
       "  3.7476420402526855e-06,\n",
       "  1.8402934074401855e-06,\n",
       "  3.3676624298095703e-06,\n",
       "  1.8551945686340332e-06],\n",
       " [4.991888999938965e-06,\n",
       "  4.827976226806641e-06,\n",
       "  1.1920928955078125e-06,\n",
       "  4.32133674621582e-06,\n",
       "  2.9206275939941406e-06]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# homemade batchNorm1 filter on test\n",
    "out_homemade = maxpool2_homemade(input_batch=out_homemade)\n",
    "# original batchNorm1 filter on test\n",
    "out_original = model_original.maxPool2(out_original)\n",
    "\n",
    "print(\"MaxPool layer 1 error over out channels: \")\n",
    "compare(out_homemade, transform_input(input_batch = out_original))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp: tensor(3.8711, dtype=torch.float64)\n",
      "tensor(0.1424, dtype=torch.float64)\n",
      "tensor(4.0135, dtype=torch.float64)\n",
      "tmp: tensor(-4.5000, dtype=torch.float64)\n",
      "tensor(0.1424, dtype=torch.float64)\n",
      "tensor(-4.3576, dtype=torch.float64)\n",
      "tensor([[ 4.0135, -3.6916],\n",
      "        [-4.3576,  6.3461]], dtype=torch.float64)\n",
      "tensor([[ 4.0135, -3.6916],\n",
      "        [-4.3576,  6.3461]], grad_fn=<AddmmBackward0>)\n",
      "Linear layer 1 error over out channels: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sture\\AppData\\Local\\Temp\\ipykernel_1688\\2926538855.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\b\\abs_bao0hdcrdh\\croot\\pytorch_1675190257512\\work\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  out_homemade = linear_homemade(torch.reshape(torch.tensor(out_homemade, dtype = torch.double),(2,45)))\n",
      "c:\\Users\\sture\\Documents\\Studie\\Bachelor\\bachelorprojekt\\CNN_layers\\linear_layer_homemade.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tmp = torch.dot(input[n],torch.tensor(self.weights[i], dtype = torch.double)) + torch.tensor(self.bias[i], dtype = torch.double)\n",
      "c:\\Users\\sture\\Documents\\Studie\\Bachelor\\bachelorprojekt\\CNN_layers\\linear_layer_homemade.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print(torch.tensor(self.bias[i], dtype = torch.double))\n",
      "c:\\Users\\sture\\Documents\\Studie\\Bachelor\\bachelorprojekt\\CNN_layers\\linear_layer_homemade.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tmp = torch.hstack((tmp,(torch.dot(input[n],torch.tensor(self.weights[i], dtype = torch.double)) + torch.tensor(self.bias[i], dtype = torch.double))))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[2.3730480247508012e-07, 4.3005196914691624e-07],\n",
       " [2.7937032829328245e-06, 1.0375646679605666e-06]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# homemade linear filter on test\n",
    "out_homemade = linear_homemade(torch.reshape(torch.tensor(out_homemade, dtype = torch.double),(2,45)))\n",
    "# original linear filter on test\n",
    "out_original = model_original.lin(torch.reshape(out_original, (2,45)))\n",
    "\n",
    "print(\"Linear layer 1 error over out channels: \")\n",
    "compare(out_homemade.numpy(), list(list(out_original.detach().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'CNN_layers.linear_layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mCNN_layers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m linear_layer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'CNN_layers.linear_layer'"
     ]
    }
   ],
   "source": [
    "from CNN_layers.linear_layer import linear_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0055, -0.3833, -0.0223, -0.1087, -0.0395, -0.1584, -0.2102, -0.3390,\n",
      "        -0.1260,  0.1047,  0.0076,  0.3647, -0.0429, -0.3491,  0.1892, -0.1851,\n",
      "        -0.1496, -0.0308,  0.3081,  0.1367, -0.1633,  0.2652, -0.1780,  0.0351,\n",
      "         0.4791, -0.0754,  0.1078, -0.1172,  0.3277,  0.2573, -0.0378, -0.2831,\n",
      "         0.0229,  0.2404,  0.1599, -0.3007, -0.1929, -0.2463, -0.3025,  0.2377,\n",
      "         0.1619, -0.2581, -0.0129, -0.2355, -0.3202],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([ 0.0055, -0.3833, -0.0223, -0.1087, -0.0395, -0.1584, -0.2102, -0.3390,\n",
      "        -0.1260,  0.1047,  0.0076,  0.3647, -0.0429, -0.3491,  0.1892, -0.1851,\n",
      "        -0.1496, -0.0308,  0.3081,  0.1367, -0.1633,  0.2652, -0.1780,  0.0351,\n",
      "         0.4791, -0.0754,  0.1078, -0.1172,  0.3277,  0.2573, -0.0378, -0.2831,\n",
      "         0.0229,  0.2404,  0.1599, -0.3007, -0.1929, -0.2463, -0.3025,  0.2377,\n",
      "         0.1619, -0.2581, -0.0129, -0.2355, -0.3202], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2866144332.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[19], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    lin_in =\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "lin = linear_layer(model_original.lin.weight,model_original.lin.bias,2)\n",
    "\n",
    "lin_in = \n",
    "\n",
    "out = lin(torch.flatten(torch.tensor(out_homemade)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "633b0076f780d719a192e9b459f0ad23c7f976dd6b3ef0dd604e5581de9baee4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
