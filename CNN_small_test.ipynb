{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural net\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "from CNN_small_architecture import CNNSmall\n",
    "import conv_homemade\n",
    "import maxpool_homemade\n",
    "import batchnorm_homemade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(num):\n",
    "    if num == 1:\n",
    "        return torch.tensor(np.array([1., 0.]))\n",
    "    else:\n",
    "        return torch.tensor(np.array([0., 1.]))\n",
    "\n",
    "MNIST_test = datasets.MNIST(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "test_set = [[data[0], tokenize(data[1])] for data in MNIST_test if data[1] in [1,2]]\n",
    "\n",
    "batch_size = 2\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a 4-dimensional tensor from the dataloader (batch_size x channel_size x height x width)\n",
    "# and turns it into a list of lists of (height x width) numpy arrays\n",
    "def transform_input(input_batch):\n",
    "    return list(list(input_batch.detach().numpy()))\n",
    "\n",
    "# compares two lists of batches with same amount of channels \n",
    "# and returns the summed absoute loss for each batch over the channels\n",
    "def compare(x, y):\n",
    "    b_diff = []\n",
    "    batch_size = len(x)\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        c_diff = []\n",
    "        channels = len(x[b])\n",
    "\n",
    "        for c in range(channels):\n",
    "            diff = np.sum(np.absolute(x[b][c] - y[b][c]))\n",
    "            c_diff.append(diff)\n",
    "        \n",
    "        b_diff.append(c_diff)\n",
    "    \n",
    "    return b_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_original = CNNSmall()\n",
    "\n",
    "path = \"CNN_small\"\n",
    "load = True\n",
    "\n",
    "if load and os.path.isfile(path):\n",
    "    model_original.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_original_test, label_test = next(iter(test_loader))\n",
    "input_homemade_test = transform_input(input_batch=input_original_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv_homemade(model_conv):\n",
    "    weights = model_conv.weight\n",
    "    biases = model_conv.bias.detach().numpy()\n",
    "\n",
    "    out_c, in_c, r, c = weights.shape\n",
    "    conv1_filters = []\n",
    "\n",
    "    for f in range(out_c):\n",
    "        filter_ = []\n",
    "        kernels = []\n",
    "\n",
    "        for kernel in list(weights[f,:,:,:]):\n",
    "            kernels.append(kernel.detach().numpy())\n",
    "\n",
    "        filter_.append(kernels)\n",
    "        filter_.append(biases[f])\n",
    "        conv1_filters.append(filter_)\n",
    "    \n",
    "    return conv_homemade.Conv(filters=conv1_filters, in_channels=in_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batchnorm_homemade(model_batchnorm):\n",
    "    weights = model_batchnorm.weight\n",
    "    biases = model_batchnorm.bias\n",
    "\n",
    "    return batchnorm_homemade.BatchNorm(weights=weights, biases=biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_maxpool_homemade(model_maxpool):\n",
    "    kernel_size = model_maxpool.kernel_size\n",
    "    stride = model_maxpool.stride\n",
    "    if type(model_maxpool.padding) == int:\n",
    "        padding = (model_maxpool.padding, model_maxpool.padding)\n",
    "    else:\n",
    "        padding = model_maxpool.padding\n",
    "    \n",
    "    return maxpool_homemade.MaxPool(kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_homemade = create_conv_homemade(model_conv=model_original.conv1)\n",
    "batchnorm1_homemade = create_batchnorm_homemade(model_batchnorm=model_original.batchNorm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional layer 1 error over out channels: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1.249955558127458e-06, 2.8312750674358567e-06, 2.762250141147282e-06],\n",
       " [2.8683681889060675e-06, 3.831799332179586e-06, 5.355794661626723e-06]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# homemade conv1 filter on test\n",
    "out_homemade = conv1_homemade(input_homemade_test)\n",
    "# original conv1 filter on test\n",
    "out_original = model_original.conv1(input_original_test)\n",
    "\n",
    "print(\"Convolutional layer 1 error over out channels: \")\n",
    "compare(out_homemade, transform_input(input_batch = out_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchNorm layer 1 error over out channels: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[101.62296838779002, 86.84006728418171, 82.94758552778512],\n",
       " [92.99368042871356, 86.79554948832083, 82.70641375286505]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# homemade batchNorm1 filter on test\n",
    "out_homemade = batchnorm1_homemade(input_batch=out_homemade)\n",
    "# original batchNorm1 filter on test\n",
    "out_original = model_original.batchNorm1(out_original)\n",
    "\n",
    "# print(out_homemade[0][0])\n",
    "# print(out_original[0][0])\n",
    "\n",
    "print(\"BatchNorm layer 1 error over out channels: \")\n",
    "compare(out_homemade, transform_input(input_batch = out_original))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f31e8003a22d1d43deb930c821e4af1d8090c97ed08fe8384c28a99b74fb445c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
