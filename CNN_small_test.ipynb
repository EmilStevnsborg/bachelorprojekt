{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural net\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "from CNN_small_architecture import CNNSmall\n",
    "from CNN_layers import conv_homemade\n",
    "from CNN_layers import maxpool_homemade\n",
    "from CNN_layers import batchnorm_homemade, batchnorm_homemade_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(num):\n",
    "    if num == 1:\n",
    "        return torch.tensor(np.array([1., 0.]))\n",
    "    else:\n",
    "        return torch.tensor(np.array([0., 1.]))\n",
    "\n",
    "MNIST_test = datasets.MNIST(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "test_set = [[data[0], tokenize(data[1])] for data in MNIST_test if data[1] in [1,2]]\n",
    "\n",
    "batch_size = 2\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a 4-dimensional tensor from the dataloader (batch_size x channel_size x height x width)\n",
    "# and turns it into a list of lists of (height x width) numpy arrays\n",
    "def transform_input(input_batch):\n",
    "    return list(list(input_batch.detach().numpy()))\n",
    "\n",
    "# compares two lists of batches with same amount of channels \n",
    "# and returns the summed absoute loss for each batch over the channels\n",
    "def compare(x, y):\n",
    "    b_diff = []\n",
    "    batch_size = len(x)\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        c_diff = []\n",
    "        channels = len(x[b])\n",
    "\n",
    "        for c in range(channels):\n",
    "            diff = np.sum(np.absolute(x[b][c] - y[b][c]))\n",
    "            c_diff.append(diff)\n",
    "        \n",
    "        b_diff.append(c_diff)\n",
    "    \n",
    "    return b_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_original = CNNSmall()\n",
    "model_original.eval()\n",
    "\n",
    "path = \"CNN_small\"\n",
    "load = True\n",
    "\n",
    "if load and os.path.isfile(path):\n",
    "    model_original.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "input_original_test, label_test = next(iter(test_loader))\n",
    "print(np.array(input_original_test).shape)\n",
    "input_homemade_test = transform_input(input_batch=input_original_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv_homemade(model_conv):\n",
    "    weights = model_conv.weight\n",
    "    biases = model_conv.bias.detach().numpy()\n",
    "\n",
    "    out_c, in_c, r, c = weights.shape\n",
    "    conv1_filters = []\n",
    "\n",
    "    for f in range(out_c):\n",
    "        filter_ = []\n",
    "        kernels = []\n",
    "\n",
    "        for kernel in list(weights[f,:,:,:]):\n",
    "            kernels.append(kernel.detach().numpy())\n",
    "\n",
    "        filter_.append(kernels)\n",
    "        filter_.append(biases[f])\n",
    "        conv1_filters.append(filter_)\n",
    "    \n",
    "    return conv_homemade.Conv(filters=conv1_filters, in_channels=in_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batchnorm_homemade(model_batchnorm):\n",
    "    weights = model_batchnorm.weight\n",
    "    biases = model_batchnorm.bias\n",
    "    running_mean = model_batchnorm.running_mean\n",
    "    running_var = model_batchnorm.running_var\n",
    "\n",
    "    return batchnorm_homemade_2.BatchNorm(weights=weights, biases=biases, running_mean = running_mean, running_var = running_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_maxpool_homemade(model_maxpool):\n",
    "    kernel_size = model_maxpool.kernel_size\n",
    "    stride = model_maxpool.stride\n",
    "    if type(model_maxpool.padding) == int:\n",
    "        padding = (model_maxpool.padding, model_maxpool.padding)\n",
    "    else:\n",
    "        padding = model_maxpool.padding\n",
    "    \n",
    "    return maxpool_homemade.MaxPool(kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_homemade = create_conv_homemade(model_conv=model_original.conv1)\n",
    "batchnorm1_homemade = create_batchnorm_homemade(model_batchnorm=model_original.batchNorm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional layer 1 error over out channels: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[4.042366603063119e-06, 3.4594775027430025e-06, 1.0153361123532711e-06],\n",
       " [6.455155075843488e-06, 5.694257553062743e-06, 1.924536561247192e-06]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# homemade conv1 filter on test\n",
    "out_homemade = conv1_homemade(input_homemade_test)\n",
    "# original conv1 filter on test\n",
    "out_original = model_original.conv1(input_original_test)\n",
    "\n",
    "print(\"Convolutional layer 1 error over out channels: \")\n",
    "compare(out_homemade, transform_input(input_batch = out_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "homemade:  [[-0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804  ]\n",
      " [-0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804  ]\n",
      " [-0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804  ]\n",
      " [-0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.83662343 -1.16544616\n",
      "  -1.46901238 -1.21680117 -1.2410506  -0.5985139  -0.359804   -0.359804\n",
      "  -0.359804   -0.359804  ]\n",
      " [-0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.72895455 -1.35915053 -0.97963774\n",
      "  -0.18445884  1.41355312  0.30217224 -0.31150141 -0.359804   -0.359804\n",
      "  -0.359804   -0.359804  ]\n",
      " [-0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.99568361 -0.90116209  0.80319196\n",
      "   1.91090357  3.16088891  1.04037821 -0.21098839 -0.359804   -0.359804\n",
      "  -0.359804   -0.359804  ]\n",
      " [-0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.62128568 -1.34576285 -0.29876733  1.51170862\n",
      "   2.93116164  3.20669603  1.80017769 -0.00804674 -0.359804   -0.359804\n",
      "  -0.359804   -0.359804  ]\n",
      " [-0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.59052306 -1.26011825 -1.15282977  0.26013523  2.54442072\n",
      "   2.99739289  2.16568422  0.57401317 -0.22049734 -0.359804   -0.359804\n",
      "  -0.359804   -0.359804  ]\n",
      " [-0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -1.04803097 -1.08429325  0.13475601  1.90027821  3.23318672\n",
      "   2.1614759   0.57813698 -0.26643628 -0.34646958 -0.359804   -0.359804\n",
      "  -0.359804   -0.359804  ]\n",
      " [-0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.43671039 -1.52826595 -0.11523662  1.34681273  2.92149305  2.89908361\n",
      "   1.14809465 -0.20312448 -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804  ]\n",
      " [-0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.48285419\n",
      "  -1.135674   -1.40188777  0.38605419  2.510952    3.1326611   2.03524685\n",
      "   0.25193217 -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804  ]\n",
      " [-0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.80889177\n",
      "  -1.29613888 -0.19200955  1.84468365  3.16316509  2.53325987  0.88550478\n",
      "  -0.24979499 -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804  ]\n",
      " [-0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.66742945 -1.32628036\n",
      "  -0.82517511  1.14986908  3.00098133  2.95784879  1.54833579 -0.08235262\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804  ]\n",
      " [-0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.48285419 -1.41245186 -0.95844561\n",
      "   0.11928532  2.29742169  2.97062087  1.43262875  0.12846842 -0.33480197\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804  ]\n",
      " [-0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.98962176 -1.29089582  0.24454695\n",
      "   1.82931697  3.09789538  2.29525304  0.64879668 -0.32146752 -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804  ]\n",
      " [-0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.54437929 -1.3685993  -0.42564136  1.46338046\n",
      "   2.47141385  3.32757902  1.60911155  0.08054657 -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804  ]\n",
      " [-0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -1.33337057 -1.01209998  0.17957617  2.06329894\n",
      "   3.1196146   2.84531856  0.9896121  -0.29479867 -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804  ]\n",
      " [-0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -1.67976213  0.3890658   1.57462752  2.81081152\n",
      "   2.90575433  1.57336211 -0.01916649 -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804  ]\n",
      " [-0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.97120959 -1.45379829  0.87667984  2.11386108  3.2290709\n",
      "   1.84665334  0.3120836  -0.33980235 -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804  ]\n",
      " [-0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.54437929 -1.41614199 -0.54722446  1.45876956  2.59785628  3.27051973\n",
      "   1.33235848 -0.10240693 -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804  ]\n",
      " [-0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.67966652 -1.25003171  0.89908963  2.05842853  3.2415235   2.52119088\n",
      "   0.9024595  -0.27216822 -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804  ]\n",
      " [-0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.69175619 -0.91802651  1.55274498  2.30634212  3.22522974  0.93911779\n",
      "  -0.08475472 -0.34646958 -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804  ]\n",
      " [-0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.53377753 -0.65955895  1.77263665  2.5574131   3.04805112  0.16233683\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804  ]\n",
      " [-0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.38002148 -0.10865732  2.03672791  3.53273416  3.70999336  0.99592704\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804  ]\n",
      " [-0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.35355517 -0.18446244  0.7520082   1.33783185  1.20855987  0.00689274\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804  ]\n",
      " [-0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804   -0.359804   -0.359804   -0.359804   -0.359804\n",
      "  -0.359804   -0.359804  ]]\n",
      "original:  tensor([[-0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293],\n",
      "        [-0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293],\n",
      "        [-0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293],\n",
      "        [-0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.7958, -1.1174, -1.4144, -1.1677, -1.1914, -0.5628, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293],\n",
      "        [-0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.6904,\n",
      "         -1.3069, -0.9357, -0.1578,  1.4054,  0.3183, -0.2821, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293],\n",
      "        [-0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.9514,\n",
      "         -0.8589,  0.8084,  1.8920,  3.1147,  1.0404, -0.1837, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293],\n",
      "        [-0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.5851, -1.2938,\n",
      "         -0.2696,  1.5015,  2.8900,  3.1596,  1.7837,  0.0148, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293],\n",
      "        [-0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.5550, -1.2100, -1.1051,\n",
      "          0.2771,  2.5117,  2.9548,  2.1412,  0.5842, -0.1930, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293],\n",
      "        [-0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -1.0026, -1.0380,  0.1545,\n",
      "          1.8816,  3.1855,  2.1371,  0.5882, -0.2380, -0.3163, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293],\n",
      "        [-0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293, -0.3293, -0.3293, -0.4045, -1.4723, -0.0901,  1.3402,\n",
      "          2.8806,  2.8586,  1.1458, -0.1760, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293],\n",
      "        [-0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293, -0.3293, -0.4497, -1.0883, -1.3487,  0.4003,  2.4790,\n",
      "          3.0871,  2.0136,  0.2691, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293],\n",
      "        [-0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293, -0.3293, -0.7686, -1.2453, -0.1652,  1.8272,  3.1170,\n",
      "          2.5008,  0.8889, -0.2217, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293],\n",
      "        [-0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293, -0.6302, -1.2748, -0.7846,  1.1475,  2.9583,  2.9161,\n",
      "          1.5373, -0.0579, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293],\n",
      "        [-0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.4497, -1.3590, -0.9149,  0.1393,  2.2701,  2.9286,  1.4241,\n",
      "          0.1483, -0.3049, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293],\n",
      "        [-0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.9454, -1.2401,  0.2619,  1.8122,  3.0531,  2.2680,  0.6573,\n",
      "         -0.2918, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293],\n",
      "        [-0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.5099, -1.3162, -0.3937,  1.4542,  2.4403,  3.2778,  1.5967,  0.1015,\n",
      "         -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293],\n",
      "        [-0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -1.2817, -0.9674,  0.1983,  2.0410,  3.0744,  2.8060,  0.9907, -0.2657,\n",
      "         -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293],\n",
      "        [-0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -1.6205,  0.4033,  1.5630,  2.7723,  2.8652,  1.5618,  0.0039, -0.3293,\n",
      "         -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293],\n",
      "        [-0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.9274,\n",
      "         -1.3995,  0.8803,  2.0905,  3.1814,  1.8291,  0.3279, -0.3097, -0.3293,\n",
      "         -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293],\n",
      "        [-0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.5099, -1.3627,\n",
      "         -0.5127,  1.4497,  2.5640,  3.2220,  1.3260, -0.0775, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293],\n",
      "        [-0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.6422, -1.2002,\n",
      "          0.9022,  2.0363,  3.1936,  2.4890,  0.9055, -0.2436, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293],\n",
      "        [-0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.6540, -0.8754,\n",
      "          1.5416,  2.2788,  3.1777,  0.9413, -0.0603, -0.3163, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293],\n",
      "        [-0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.4995, -0.6225,\n",
      "          1.7567,  2.5244,  3.0044,  0.1815, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293],\n",
      "        [-0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3491, -0.0836,\n",
      "          2.0151,  3.4785,  3.6519,  0.9969, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293],\n",
      "        [-0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3232, -0.1578,\n",
      "          0.7583,  1.3314,  1.2049,  0.0294, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293],\n",
      "        [-0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293, -0.3293,\n",
      "         -0.3293, -0.3293]], grad_fn=<SelectBackward0>)\n",
      "BatchNorm layer 1 error over out channels: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[20.581934231799096, 19.10204150690697, 30.70032871537842],\n",
       " [20.32747952779755, 27.384879199787974, 37.95119834225625]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# homemade batchNorm1 filter on test\n",
    "out_homemade = batchnorm1_homemade(input=out_homemade)\n",
    "# original batchNorm1 filter on test\n",
    "out_original = model_original.batchNorm1(out_original)\n",
    "\n",
    "print(\"BatchNorm layer 1 error over out channels: \")\n",
    "compare(out_homemade, transform_input(input_batch = out_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "633b0076f780d719a192e9b459f0ad23c7f976dd6b3ef0dd604e5581de9baee4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
