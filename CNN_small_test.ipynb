{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing homemade layers\n",
    "\n",
    "This notebook contains tests for all the homemade layers. The tests takes some samples from the mni-\n",
    "st dataset as a minibatch and compares the results from running this minibatch through the test CNN\n",
    "to running the same minibatch through the homemade layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing relevant libraries\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "from CNN_small_architecture import CNNSmall\n",
    "from CNN_layers import conv_homemade\n",
    "from CNN_layers import maxpool_homemade\n",
    "from CNN_layers import batchnorm_homemade\n",
    "from CNN_layers import linear_layer_homemade\n",
    "from CNN_layers import elu_homemade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emil/anaconda3/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "def tokenize(num):\n",
    "    if num == 1:\n",
    "        return torch.tensor(np.array([1., 0.]))\n",
    "    else:\n",
    "        return torch.tensor(np.array([0., 1.]))\n",
    "\n",
    "MNIST_test = datasets.MNIST(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "test_set = [[data[0], tokenize(data[1])] for data in MNIST_test if data[1] in [1,2]]\n",
    "\n",
    "batch_size = 2\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a 4-dimensional tensor from the dataloader (batch_size x channel_size x height x width)\n",
    "# and turns it into a list of lists of (height x width) numpy arrays\n",
    "def transform_input(input_batch):\n",
    "    return list(list(input_batch.detach().numpy()))\n",
    "\n",
    "# compares two lists of batches with same amount of channels \n",
    "# and returns the summed absoute loss for each batch over the channels\n",
    "def compare(x, y):\n",
    "    b_diff = []\n",
    "    batch_size = len(x)\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        c_diff = []\n",
    "        channels = len(x[b])\n",
    "\n",
    "        for c in range(channels):\n",
    "            diff = np.sum(np.absolute(x[b][c] - y[b][c]))\n",
    "            c_diff.append(diff)\n",
    "        \n",
    "        b_diff.append(c_diff)\n",
    "    \n",
    "    return b_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_original = CNNSmall()\n",
    "model_original.eval()\n",
    "\n",
    "path = \"CNN_small\"\n",
    "load = True\n",
    "\n",
    "if load and os.path.isfile(path):\n",
    "    model_original.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_original_test, label_test = next(iter(test_loader))\n",
    "input_homemade_test = transform_input(input_batch=input_original_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv_homemade(model_conv):\n",
    "    weights = model_conv.weight\n",
    "    biases = model_conv.bias.detach().numpy()\n",
    "\n",
    "    out_c, in_c, r, c = weights.shape\n",
    "    conv1_filters = []\n",
    "\n",
    "    for f in range(out_c):\n",
    "        filter_ = []\n",
    "        kernels = []\n",
    "\n",
    "        for kernel in list(weights[f,:,:,:]):\n",
    "            kernels.append(kernel.detach().numpy())\n",
    "\n",
    "        filter_.append(kernels)\n",
    "        filter_.append(biases[f])\n",
    "        conv1_filters.append(filter_)\n",
    "    \n",
    "    return conv_homemade.Conv(filters=conv1_filters, in_channels=in_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batchnorm_homemade(model_batchnorm):\n",
    "    weights = model_batchnorm.weight\n",
    "    biases = model_batchnorm.bias\n",
    "    running_mean = model_batchnorm.running_mean\n",
    "    running_var = model_batchnorm.running_var\n",
    "\n",
    "    return batchnorm_homemade.BatchNorm(weights=weights, biases=biases, running_mean = running_mean, running_var = running_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_maxpool_homemade(model_maxpool):\n",
    "    kernel_size = model_maxpool.kernel_size\n",
    "    stride = model_maxpool.stride\n",
    "    if type(model_maxpool.padding) == int:\n",
    "        padding = (model_maxpool.padding, model_maxpool.padding)\n",
    "    else:\n",
    "        padding = model_maxpool.padding\n",
    "    \n",
    "    return maxpool_homemade.MaxPool(kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRIDE ER PRÆDEFINERET\n",
      "STRIDE ER PRÆDEFINERET\n"
     ]
    }
   ],
   "source": [
    "elu_homemade = elu_homemade.ELU()\n",
    "conv1_homemade = create_conv_homemade(model_conv= model_original.conv1)\n",
    "batchnorm1_homemade = create_batchnorm_homemade(model_batchnorm= model_original.batchNorm1)\n",
    "maxpool1_homemade = create_maxpool_homemade(model_maxpool= model_original.maxPool1)\n",
    "conv2_homemade = create_conv_homemade(model_conv= model_original.conv2)\n",
    "batchnorm2_homemade = create_batchnorm_homemade(model_batchnorm= model_original.batchNorm2)\n",
    "maxpool2_homemade = create_maxpool_homemade(model_original.maxPool2)\n",
    "linear_homemade = linear_layer_homemade.linear_layer(model_original.lin.weight,model_original.lin.bias,2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the first convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional layer 1 error over out channels: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1.249955558127458e-06, 2.8312750674358567e-06, 2.762250141147282e-06],\n",
       " [2.8683681889060675e-06, 3.831799332179586e-06, 5.355794661626723e-06]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# homemade conv1 filter on test\n",
    "out_homemade = conv1_homemade(input_homemade_test)\n",
    "# original conv1 filter on test\n",
    "out_original = model_original.conv1(input_original_test)\n",
    "\n",
    "print(\"Convolutional layer 1 error over out channels: \")\n",
    "compare(out_homemade, transform_input(input_batch = out_original))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the first Batchnorm layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchNorm layer 1 error over out channels: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[62.87329353426189, 102.12569214145174, 48.019551429379696],\n",
       " [175.5036352051182, 164.28217146854848, 44.70813735950847]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# homemade batchNorm1 filter on test\n",
    "out_homemade = elu_homemade(batchnorm1_homemade(input_batch=out_homemade))\n",
    "# original batchNorm1 filter on test\n",
    "out_original = model_original.batchNorm1(out_original)\n",
    "\n",
    "print(\"BatchNorm layer 1 error over out channels: \")\n",
    "compare(out_homemade, transform_input(input_batch = out_original))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the first Maxpool layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxPool layer 1 error over out channels: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emil/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[4.934036563136274, 11.104199516140637, 10.444550115584917],\n",
       " [10.277595664672054, 14.890164434362028, 8.153242071128068]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# homemade MaxPool filter on test\n",
    "out_homemade = maxpool1_homemade(input_batch=out_homemade)\n",
    "# original MaxPool filter on test\n",
    "out_original = model_original.maxPool1(out_original)\n",
    "\n",
    "print(\"MaxPool layer 1 error over out channels: \")\n",
    "compare(out_homemade, transform_input(input_batch = out_original))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the second Convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxPool layer 1 error over out channels: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[10.389811412095373,\n",
       "  7.975448856507968,\n",
       "  6.612556507253572,\n",
       "  5.5012117906674876,\n",
       "  5.6373320385733585],\n",
       " [12.448959763971265,\n",
       "  12.944105723835168,\n",
       "  7.879914341820375,\n",
       "  8.671488429027105,\n",
       "  8.914771907028012]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: test the second convolutional layer\n",
    "\n",
    "# homemade batchNorm1 filter on test\n",
    "out_homemade = conv2_homemade(input_batch=out_homemade)\n",
    "# original batchNorm1 filter on test\n",
    "out_original = model_original.conv2(out_original)\n",
    "\n",
    "print(\"MaxPool layer 1 error over out channels: \")\n",
    "compare(out_homemade, transform_input(input_batch = out_original))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the second Batchnorm layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxPool layer 1 error over out channels: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[42.05489564819909,\n",
       "  27.903594782175663,\n",
       "  24.360559965050626,\n",
       "  29.39420929903927,\n",
       "  15.734080881532082],\n",
       " [55.85142271821543,\n",
       "  44.0320721840459,\n",
       "  33.56394753824635,\n",
       "  31.616508368874822,\n",
       "  31.787381688712667]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# homemade batchNorm1 filter on test\n",
    "out_homemade = elu_homemade(batchnorm2_homemade(input_batch=out_homemade))\n",
    "# original batchNorm1 filter on test\n",
    "out_original = model_original.batchNorm2(out_original)\n",
    "\n",
    "print(\"MaxPool layer 1 error over out channels: \")\n",
    "compare(out_homemade, transform_input(input_batch = out_original))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the second Maxpool layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxPool layer 1 error over out channels: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[2.4126090109348297,\n",
       "  1.78065165372394,\n",
       "  3.0777494387606352,\n",
       "  2.0288141478527573,\n",
       "  2.2179447000583576],\n",
       " [2.6288576126098633,\n",
       "  2.1977109909057617,\n",
       "  2.946653187274933,\n",
       "  1.587695837020874,\n",
       "  2.685471463479278]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# homemade batchNorm1 filter on test\n",
    "out_homemade = maxpool2_homemade(input_batch=out_homemade)\n",
    "# original batchNorm1 filter on test\n",
    "out_original = model_original.maxPool2(out_original)\n",
    "\n",
    "print(\"MaxPool layer 1 error over out channels: \")\n",
    "compare(out_homemade, transform_input(input_batch = out_original))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear layer 1 error over out channels: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emil/Documents/bachelor/bachelorprojekt/CNN_layers/linear_layer_homemade.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tmp = torch.dot(input[n],torch.tensor(self.weights[i], dtype = torch.double)) + torch.tensor(self.bias[i], dtype = torch.double)\n",
      "/home/emil/Documents/bachelor/bachelorprojekt/CNN_layers/linear_layer_homemade.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tmp = torch.hstack((tmp,(torch.dot(input[n],torch.tensor(self.weights[i], dtype = torch.double)) + torch.tensor(self.bias[i], dtype = torch.double))))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.21692568934417, 0.2640675454205441],\n",
       " [0.39087817855161866, 0.5113737620091072]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# homemade linear filter on test\n",
    "out_homemade = linear_homemade(torch.reshape(torch.tensor(out_homemade, dtype = torch.double),(2,45)))\n",
    "# original linear filter on test\n",
    "out_original = model_original.lin(torch.reshape(out_original, (2,45)))\n",
    "\n",
    "print(\"Linear layer 1 error over out channels: \")\n",
    "compare(out_homemade.numpy(), list(list(out_original.detach().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'CNN_layers.linear_layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_113088/3338103441.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mCNN_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinear_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'CNN_layers.linear_layer'"
     ]
    }
   ],
   "source": [
    "from CNN_layers.linear_layer import linear_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = linear_layer(model_original.lin.weight,model_original.lin.bias,2)\n",
    "\n",
    "lin_in = \n",
    "\n",
    "out = lin(torch.flatten(torch.tensor(out_homemade)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fabe24d4f4f8f1cb4042d3ae8581d90a8322d64984e6763308b9bfeee36ebe98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
